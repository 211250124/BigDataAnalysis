\documentclass{article}

\usepackage{amsmath} % 用于数学公式
\usepackage{graphicx} % 用于插入图片
\usepackage{lipsum} % 用于生成虚拟文本
\usepackage{ctex} % 导入 ctex 包以支持中文
\usepackage{titlesec} % 导入 titlesec 包以定制标题样式
\usepackage{fontspec} % 用于设置中文字体
\usepackage{amsfonts} 

\setmainfont{SimSun} % 设置中文字体，SimSun 为宋体的系统字体

\title{LinkPrediction实验}
\author{程智镝、陈凌}
\date{\today}

\begin{document}
\maketitle

\section*{作业任务}
\begin{itemize}
    \item 从代码（自己实现or复现）、数据集（直接获取或自己处理得到）两个角度权衡是否选择某个link prediction的工作。。
    \item 论文摘要abstract和introduction翻译
    \item 问题描述。
    \item 输入、输出、模型算法描述（附框架图；有多个的挑1个主要实现）
    \item 评价指标及其计算公式
    \item 对比方法及这些对比方法的引用论文出处
    \item 结果
    \item 打包提交code、运行配置说明（数据集太大的可以是开放链接，需描述）
\end{itemize}
\section*{实验难点：}
\begin{itemize}
    \item 论文为全英文描述，阅读难度提升
    \item 论文实验复现环境搭配
    \item 相关神经网络、机器学习、图论的知识暂且未知
\end{itemize}
\section*{实验准备工作}
依赖下载：
cuda=10.2
pytorch=1.12.1
torch-cluster=1.6.0+pt112cu102
torch-scatter=2.1.0+pt112cu102
torch-sparse=0.6.16+pt112cu102
dgl-cu102=0.9.1.post1
ogb=1.3.5
numpy=1.20.2
\section*{论文：GIPA: A General Information Propagation
Algorithm for Graph Learning}
\subsection*{abstract和introduction翻译}

\textbf{摘要：}图神经网络已经广泛应用于图结构数据计算，在节点类、链路预测和网络推荐等各种应用中显示出良好性能
现有工作主要集中在基于注意力对相邻节点进行加权聚合时的节点相关性，例如两个节点的密集向量的点积。这可能导致在进行信息传播时
要传播的节点中的冲突噪声。为了解决这个问题，我们提出了一种通用信息传播算法（GIPA），该算法利用了更细粒度的信息融合，
包括基于边缘特征的比特和特征相关性。具体地，逐位相关通过多层感知器（MLP）基于两个节点及其边缘的密集表示来计算逐元素注意力权重；
按特征的相关性基于用于特征选择的节点属性特征的一次热表示。我们在阿里巴巴集团的开放图基准蛋白数据集
和支付宝数据集上评估了GIPA的性能。结果表明GIPA在预测精度方面优于最先进的模型。例如，GIPA的平均ROC-AUC为0.8917±0.0007，优于
OGBN蛋白质排行榜上列出的所有现有方法。
关键词：图神经网络，细粒度信息融合，逐位和特征关注\\

\textbf{引言：}图表示学习通常旨在基于图拓扑（链接）信息为每个图节点学习信息嵌入。通常的，
节点的嵌入表示为低维特征向量，可用于促进下游应用。本文主要研究只有一类节点和一类边的齐次图。
目的是从图拓扑中学习节点表示。具体地，给定节点u，使用广度优先搜索、深度优先搜索或随机游动来识别一组相邻节点。
然后通过最大化u及其邻居的共现概率来学习u的嵌入。关于图嵌入的早期研究从图中捕获相邻信息的能力有限，
因为他们基于SkippGram等浅层学习模型。此外，在这些图嵌入方法中使用了转导学习，不能将其推广到训练图中不存在的新节点。\\
图神经网络是为了克服传统图嵌入模型的局限性而提出的。GNN（图神经网络）使用使用深度神经网络来句和来自相邻节点的特征信息，
并从而具有获得更好的聚合嵌入的潜力。GNN可以支持归纳学习，并在预测的过程中推断出看不见节点的类标签。
GNN的成功主要归功于领域信息的聚合。然而，GNN面临两个挑战：目标结点的那些相邻节点参与消息传递？以及每个相邻节点对聚合嵌入的贡献有多大？
对于前一个问题，对于大的稠密图或者幂律图，提出了邻域采样。
对于后者，邻居重要性估计用于在特征传播期间将不同的权重附加到不同的邻居节点。重要性抽样和注意力是两种流行的技术。
重要性采样是邻域采样的一种特殊情况，其中相邻节点的重要性权重是从结点上的分布中提取的。该分布可以从归一化拉普拉斯矩阵中导出，
也可以与GNN联合学习。利用这种分布，在每一步对邻居的自己进行采样，并用重要性权重进行聚合。与重要性抽样类似，注意力也将重要性权重赋予邻居。
然而，注意力不同于重要性抽样。注意力被表示为神经网络，并且总是作为GNN模型的一部分来学习。相反，重要性采样算法使用没有可训练参数的统计模型。\\
现有的注意力机制只考虑节点的相关性，忽略了传输噪声信息的抑制和边缘特征的信息。在现实世界的应用程序中，只有部分用户授权系统收集他们的配置文件。
该模型无法了解已配置文件的用户和我们一无所知的用户之间的节点相关性。因此，现有模型会传播噪声信息，导致节点表示不准确。
然而，两个经常相互转账的用户只进行了几次对话的用户具有不同的相关性。\\
为了解决上述问题，本文提出了一种新的图神经网络注意力模型，即广义信息传播模型（GIPA）我们设计了一个位相关模块和一个特征相关模块。具体而言，我们认为密集向量的每个维度表示节点的一个特征。因此，位相关模块在密集表示级别进行过滤。注意力权重的维度与密度向量相等。此外，我们将节点的每个属性特征表示为一个独热向量。特征相关模块通过输出具有相似维度和属性特征的注意力权重来执行特征选择。值得一提的是，为了使模型能够提取更好的注意力权重，度量节点之间相关性的边缘特征也包括在注意力计算中。最后，GIPA将稀疏嵌入和密集嵌入输入到深度神经网络的宽端和深端，分别用于学习特定任务。我们的贡献总结如下：

1) 我们设计了位相关模块和特征相关模块，以从元素级和特征级执行更精细的信息加权聚合，并利用了边缘信息。

2) 基于宽深度架构[6]，我们使用密集特征表示和稀疏特征表示分别提取深层信息和保留浅层原始信息，为下游任务提供更全面的信息。

3) 在开放图基准（OGB）[11]蛋白质数据集（OGBN-proteins）上的实验证明，GIPA在OGBN-proteins排行榜上取得了更好的准确性，平均ROC-AUC为0.8917±0.0007，优于排行榜上列出的最先进方法。此外，GIPA已在亿级规模的支付宝工业数据集上进行了测试。
\section*{问题描述}


\section*{链路预测--Link Prediction}
\subsection*{图论前导知识}
假设有图 $G = (V, E, A)$，其中 $V$ 为图的节点集合，
$E$ 为图的边集合，而张量 $A \in \mathbb{R}^{n \times n \times d}$ 
包含了所有节点的属性（例如，用户资料）和边的属性（例如，交互作用的强度或类型）。对于每个节点 $v \in V$，它的属性（如果有的话）存储在对角组件 $A_{vv}$ 中，而非对角线组件 $A_{uv}$ 则可以包含边 $(u, v)$ 的属性，如果 $(u, v) \in E$；否则 $A_{uv} = 0$。
\subsection*{链路预测问题}
在链路预测中的目标是根据观察到的张量 $A$ 推断目标节点对之间是否存在边。学习问题是找到一个似然（或评分）函数 $f$，它为每对目标节点$(u, v) \notin E$ 分配交互似然性（分数） $\hat{A}_{uv}$，这些节点之间的关系未被观察到。较大的 $\hat{A}_{uv}$ 表示 $(u, v)$ 形成链接或缺失链接的可能性较高。函数 $f$ 可以表示为 $\hat{A}_{uv} = f(u, v, A|\Theta)$，其中 $\Theta$ 表示模型参数。


\end{document}